# Transformer-Translation-DL

## Overview

This project implements a **Transformer model** from scratch for a basic **language translation task** as part of the Deep Learning & Neural Networks course at Mälardalen University (MDU), 2025.
The goal was to gain hands-on experience with the Transformer architecture, which underpins many modern NLP models such as BERT and GPT.

---

## Objectives

* Understand and implement the **core components** of the Transformer architecture:

  * Multi-Head Self-Attention
  * Positional Encoding
  * Encoder and Decoder stacks
* Train a Transformer on a **toy dataset for machine translation**.
* Evaluate translation quality and compare to simpler baseline models.

---

## Tech Stack

* Python
* PyTorch
* Jupyter Notebook

---

## Key Learnings

* How self-attention captures dependencies in sequences better than RNNs.
* Importance of positional encoding for handling word order.
* How Transformers enable parallel training and scale to large datasets.

---

## Repository Structure

```
MDU_DL_Transformers_Lab4.ipynb   # Main Jupyter Notebook with implementation
README.md                        # Project documentation
```

---

## Course Context

This lab was part of:
**DVA307 – Deep Learning and Neural Networks**
Mälardalen University, 2025

Assignment 4: *Implement a simple Transformer for language translation*.

---


## License

MIT License (or specify your choice).

---

